# =============================================================================
# Multi-Agent Customer Support - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your API keys

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Supported providers: 'gemini' or 'groq'
LLM_PROVIDER=gemini

# =============================================================================
# API Keys (set the one for your chosen provider)
# =============================================================================
# Google Gemini API Key (required if LLM_PROVIDER=gemini)
GEMINI_API_KEY=your_gemini_api_key_here

# Groq API Key (required if LLM_PROVIDER=groq)
GROQ_API_KEY=your_groq_api_key_here

# =============================================================================
# Model Configuration
# =============================================================================
# Gemini models (used when LLM_PROVIDER=gemini)
# PRIMARY_MODEL: Used for complex tasks (Triage, Action, Escalation)
# SECONDARY_MODEL: Used for simpler tasks (Knowledge, Follow-up) - more cost-efficient
GEMINI_PRIMARY_MODEL=gemini-2.0-flash
GEMINI_SECONDARY_MODEL=gemini-2.0-flash

# Groq models (used when LLM_PROVIDER=groq)
# Available models: llama-3.1-8b-instant, llama-3.3-70b-versatile, mixtral-8x7b-32768
GROQ_PRIMARY_MODEL=llama-3.1-8b-instant
GROQ_SECONDARY_MODEL=llama-3.1-8b-instant

# =============================================================================
# Embedding Configuration (for RAG/Vector Search)
# =============================================================================
# Currently only supports Gemini embeddings
EMBEDDINGS_PROVIDER=gemini
EMBEDDING_MODEL=models/text-embedding-004

# =============================================================================
# Generation Parameters
# =============================================================================
GEN_TEMPERATURE=0.7
GEN_MAX_OUTPUT_TOKENS=2048
GEN_TOP_P=0.95
GEN_TOP_K=40

# =============================================================================
# Rate Limit Configuration (for evaluations)
# =============================================================================
# Delay between API calls during evaluation (seconds)
RATE_LIMIT_DELAY=2.0

# Wait time when rate limit is hit (seconds) - will use exponential backoff
RATE_LIMIT_RETRY_DELAY=20.0

# Maximum number of retries when rate limit is hit
RATE_LIMIT_MAX_RETRIES=3

# =============================================================================
# LangSmith Tracing (Optional - for debugging and monitoring)
# =============================================================================
LANGCHAIN_TRACING_V2=false
LANGCHAIN_API_KEY=your_langsmith_api_key_here
LANGCHAIN_PROJECT=multi-agent-customer-support
